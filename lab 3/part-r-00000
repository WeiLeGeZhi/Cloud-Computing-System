!=	3
""	7
"".	4
"$HADOOP_CLASSPATH"	1
"$HADOOP_HEAPSIZE"	1
"$JAVA_HOME"	2
"$YARN_HEAPSIZE"	1
"$YARN_LOGFILE"	1
"$YARN_LOG_DIR"	1
"$YARN_POLICYFILE"	1
"*"	18
"AS	21
"Error:	1
"License");	21
"alice,bob	18
"clumping"	1
"console"	1
"dfs"	3
"hadoop.root.logger".	1
"jks".	4
"jvm"	3
"mapred"	3
"rpc"	3
"run	1
"ugi"	3
"x"	1
"x$JAVA_LIBRARY_PATH"	1
#	394
#!/bin/bash	2
###	4
#*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40	1
#*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both	1
#*.sink.ganglia.tagsForPrefix.dfs=	1
#*.sink.ganglia.tagsForPrefix.jvm=ProcesName	1
#*.sink.ganglia.tagsForPrefix.mapred=	1
#*.sink.ganglia.tagsForPrefix.rpc=	1
#A	1
#Default	1
#HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData	1
#Security	1
#The	1
#datanode.sink.file.filename=datanode-metrics.out	1
#datanode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#datanode.webhdfs.logger=INFO,console	1
#dfs.class=org.apache.hadoop.metrics.file.FileContext	1
#dfs.fileName=/tmp/dfsmetrics.log	1
#dfs.period=10	1
#echo	1
#export	12
#jobhistoryserver.sink.file.filename=jobhistoryserver-metrics.out	1
#jobhistoryserver.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#jvm.class=org.apache.hadoop.metrics.file.FileContext	1
#jvm.class=org.apache.hadoop.metrics.spi.NullContext	1
#jvm.fileName=/tmp/jvmmetrics.log	1
#jvm.period=10	1
#log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false	1
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd	1
#log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log	1
#log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout	1
#log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender	1
#log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd	1
#log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log	1
#log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout	1
#log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender	1
#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log	1
#log4j.appender.datanoderequestlog.RetainDays=3	1
#log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log	1
#log4j.appender.jobhistoryrequestlog.RetainDays=3	1
#log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log	1
#log4j.appender.namenoderequestlog.RetainDays=3	1
#log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log	1
#log4j.appender.nodemanagerrequestlog.RetainDays=3	1
#log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log	1
#log4j.appender.resourcemanagerrequestlog.RetainDays=3	1
#log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.logger.BlockStateChange=DEBUG	1
#log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}	1
#log4j.logger.http.requests.datanode=INFO,datanoderequestlog	1
#log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog	1
#log4j.logger.http.requests.namenode=INFO,namenoderequestlog	1
#log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog	1
#log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog	1
#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG	1
#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG	1
#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG	1
#log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}	1
#mapred.class=org.apache.hadoop.metrics.file.FileContext	1
#mapred.fileName=/tmp/mrmetrics.log	1
#mapred.period=10	1
#mapreduce.hs.audit.logger=INFO,HSAUDIT	1
#mrappmaster.sink.file.filename=mrappmaster-metrics.out	1
#mrappmaster.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#namenode.sink.*.period=8	1
#namenode.sink.file.filename=namenode-metrics.out	1
#namenode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#nodemanager.sink.file.filename=nodemanager-metrics.out	1
#nodemanager.sink.file_jvm.class=org.apache.hadoop.metrics2.sink.FileSink	1
#nodemanager.sink.file_jvm.context=jvm	1
#nodemanager.sink.file_jvm.filename=nodemanager-jvm-metrics.out	1
#nodemanager.sink.file_mapred.class=org.apache.hadoop.metrics2.sink.FileSink	1
#nodemanager.sink.file_mapred.context=mapred	1
#nodemanager.sink.file_mapred.filename=nodemanager-mapred-metrics.out	1
#nodemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#resourcemanager.sink.file.filename=resourcemanager-metrics.out	1
#resourcemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#rpc.class=org.apache.hadoop.metrics.file.FileContext	1
#rpc.fileName=/tmp/rpcmetrics.log	1
#rpc.period=10	1
#ugi.class=org.apache.hadoop.metrics.file.FileContext	1
#ugi.fileName=/tmp/ugimetrics.log	1
#ugi.period=10	1
#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY	1
$0	1
$HADOOP_CLIENT_OPTS"	1
$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*	1
$HADOOP_HOME/contrib/capacity-scheduler/*.jar;	1
$HADOOP_HOME/logs	1
$HADOOP_JAVA_PLATFORM_OPTS"	1
$HADOOP_MAPRED_HOME/logs	1
$HADOOP_PORTMAP_OPTS"	1
$JAVA_HOME"	1
$USER	2
${HADOOP_DATANODE_OPTS}	1
${HADOOP_NAMENODE_OPTS}"	1
${HTTPFS_HTTP_PORT}	1
${KMS_HTTP_PORT}	1
%-5p	3
%5p	2
%HADOOP_CLIENT_OPTS%	1
%HADOOP_DATANODE_OPTS%	1
%HADOOP_HOME%/logs	1
%HADOOP_HOME%\contrib\capacity-scheduler	1
%HADOOP_JAVA_PLATFORM_OPTS%"	1
%HADOOP_NAMENODE_OPTS%	1
%HADOOP_SECONDARYNAMENODE_OPTS%	1
%USERNAME%	1
%X{op}	2
%YARN_HEAPSIZE%	1
%c:	5
%c{1}	2
%c{2}	2
%c{2}:	6
%m%n	20
%p	11
&	2
&quot;kerberos&quot;.	1
&quot;simple&quot;	1
'	3
'${httpfs.home}/logs'	1
'${kms.home}/logs'	1
'(i.e.	2
'*'	1
'*',	2
'.	1
':'	1
'HTTP/'	1
'httpfs.log.dir'	1
'kms.log.dir'	1
'none'	1
'queue'	1
'queues'	1
'random'	1
'sasl'	1
'string'	1
'zookeeper'	2
'zookeeper'.	1
(	14
(%F:%M(%L))	2
(10	2
(APR)	1
(ASF)	11
(For	1
(Hadoop	1
(Kerberos).	1
(default)	1
(default),	1
(former)	2
(fs,	2
(in	1
(latter)	2
(resource	2
(root	1
(specified	1
(the	21
)	14
**MUST	1
**MUST**	1
*.period=10	1
*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink	1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30	1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31	1
*.sink.ganglia.period=10	1
*.sink.ganglia.supportsparse=true	1
+	2
+'%Y%m%d%H%M'`	2
-	9
-----BEGIN	1
-----END	1
-->	31
-Ddatanode.webhdfs.logger=INFO,HTTPDRFA	1
-Dfile.encoding=UTF-8"	1
-Dhadoop.home.dir=%HADOOP_YARN_HOME%	1
-Dhadoop.log.dir=$YARN_LOG_DIR"	1
-Dhadoop.log.dir=%YARN_LOG_DIR%	1
-Dhadoop.log.file=$YARN_LOGFILE"	1
-Dhadoop.log.file=%YARN_LOGFILE%	1
-Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}"	1
-Dhadoop.root.logger=%YARN_ROOT_LOGGER%	1
-Dhadoop.security.logger=INFO,DRFAS	2
-Dhdfs.audit.logger=%HDFS_AUDIT_LOGGER%	2
-Dhdfs.audit.logger=INFO,RFAAUDIT	1
-Dhdfs.audit.logger=INFO,RFAAUDIT"	1
-Djava.library.path=$JAVA_LIBRARY_PATH"	1
-Djava.library.path=%JAVA_LIBRARY_PATH%	1
-Djava.net.preferIPv4Stack=true	1
-Djava.net.preferIPv4Stack=true"	1
-Dorg.mortbay.jetty.Request.maxFormContentSize=-1	1
-Dsun.security.krb5.debug=true	1
-Dsun.security.spnego.debug"	1
-Dyarn.home.dir=$YARN_COMMON_HOME"	1
-Dyarn.home.dir=%HADOOP_YARN_HOME%	1
-Dyarn.id.str=$YARN_IDENT_STRING"	1
-Dyarn.id.str=%YARN_IDENT_STRING%	1
-Dyarn.log.dir=$YARN_LOG_DIR"	1
-Dyarn.log.dir=%YARN_LOG_DIR%	1
-Dyarn.log.file=$YARN_LOGFILE"	1
-Dyarn.log.file=%YARN_LOGFILE%	1
-Dyarn.policy.file=$YARN_POLICYFILE"	1
-Dyarn.policy.file=%YARN_POLICYFILE%	1
-Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}"	1
-Dyarn.root.logger=%YARN_ROOT_LOGGER%	1
-XX:+PrintGCDateStamps	2
-XX:+PrintGCDetails	2
-XX:+PrintGCTimeStamps	2
-XX:+UseCMSInitiatingOccupancyOnly	1
-XX:+UseCMSInitiatingOccupancyOnly"	1
-XX:+UseConcMarkSweepGC	2
-XX:CMSInitiatingOccupancyFraction=70	2
-XX:ErrorFile=${hdfs_log_dir_prefix}/hs_err_pid%p.log	2
-XX:MaxNewSize=${namenode_opt_maxnewsize}	1
-XX:MaxNewSize=200m	1
-XX:NewSize=${namenode_opt_newsize}	1
-XX:NewSize=200m	1
-XX:OnOutOfMemoryError=\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\"	1
-XX:ParallelGCThreads=4	1
-XX:ParallelGCThreads=8	1
-Xloggc:$hdfs_log_dir_prefix/gc.log-`date	1
-Xloggc:${hdfs_log_dir_prefix}/gc.log-`date	1
-Xms${dtnode_heapsize}	1
-Xms${namenode_heapsize}	1
-Xmx${dtnode_heapsize}	1
-Xmx${namenode_heapsize}	1
-Xmx1000m,	3
-f`	1
-verbose:gc	2
/tmp	3
0.	1
0.0	1
1	1
1-MAX_INT.	1
1.	1
1.0.	1
1000.	5
10000	2
1`	2
2.0	21
3.0	3
3.1	3
40.	1
5gb.	1
64MB	2
69akHZowefqRsWrEk4Rj1EAvElKAxNVkeClnipsCgYA+vN91tq80RUSuHDI7ykGv	1
6asnYzVXUz1lo6JgWnBotG5AflozEAjgG1gDSR2LFitHdPGArW2c95HIpMXI1/I0	1
8U9yixgQC6ostrqqyGomMwKBgHzLuGEhVRJ0Rvw/2YUa26ns//6YkWhPM1vbA7zc	1
9r0u+LYTvWAfGsESavx/3SyN0lkHTaqgTLCS3QIDAQABAoIBAHMdWUIDL1K2WruF	1
:	1
<!--	31
</acl-administer-jobs>	1
</acl-submit-job>	1
</body>	1
</configuration>	12
</description>	60
</html>	1
</properties>	2
</property>	220
</queue>	3
</queues>	1
</table>	1
</tr>	2
</value>	1
</xsl:for-each>	1
</xsl:stylesheet>	1
</xsl:template>	1
<?xml	13
<?xml-stylesheet	5
<LEVEL>,RMSUMMARY	1
<acl-administer-jobs>	1
<acl-submit-job>	1
<body>	1
<configuration>	12
<description>	42
<description>64G.	1
<description>ACL	21
<description>Classpath	1
<description>Default	1
<description>Keystore	2
<description>Must	2
<description>Number	1
<description>Optional.	9
<description>SSH	1
<description>Truststore	4
<html>	1
<name>default.key.acl.DECRYPT_EEK</name>	1
<name>default.key.acl.GENERATE_EEK</name>	1
<name>default.key.acl.MANAGEMENT</name>	1
<name>default.key.acl.READ</name>	1
<name>default</name>	1
<name>dfs.client.failover.proxy.provider.Ucluster</name>	1
<name>dfs.client.read.shortcircuit</name>	1
<name>dfs.data.dir</name>	1
<name>dfs.datanode.handler.count</name>	1
<name>dfs.datanode.max.transfer.threads</name>	1
<name>dfs.datanode.max.xcievers</name>	1
<name>dfs.domain.socket.path</name>	1
<name>dfs.ha.automatic-failover.enabled</name>	1
<name>dfs.ha.fencing.methods</name>	1
<name>dfs.ha.fencing.ssh.connect-timeout</name>	1
<name>dfs.ha.fencing.ssh.private-key-files</name>	1
<name>dfs.ha.namenodes.Ucluster</name>	1
<name>dfs.hosts.exclude</name>	1
<name>dfs.image.compress</name>	1
<name>dfs.journalnode.edits.dir</name>	1
<name>dfs.name.dir</name>	1
<name>dfs.namenode.handler.count</name>	1
<name>dfs.namenode.heartbeat.recheck-interval</name>	1
<name>dfs.namenode.http-address.Ucluster.nn1</name>	1
<name>dfs.namenode.http-address.Ucluster.nn2</name>	1
<name>dfs.namenode.num.checkpoints.retained</name>	1
<name>dfs.namenode.rpc-address.Ucluster.nn1</name>	1
<name>dfs.namenode.rpc-address.Ucluster.nn2</name>	1
<name>dfs.namenode.shared.edits.dir</name>	1
<name>dfs.nameservices</name>	1
<name>dfs.replication</name>	1
<name>dfs.socket.timeout</name>	1
<name>dfs.webhdfs.enabled</name>	1
<name>fs.AbstractFileSystem.us3.impl</name>	1
<name>fs.default.name</name>	1
<name>fs.trash.interval</name>	1
<name>fs.us3.access.key</name>	1
<name>fs.us3.async.wio.parallel</name>	1
<name>fs.us3.async.wio.use</name>	1
<name>fs.us3.endpoint</name>	1
<name>fs.us3.impl</name>	1
<name>fs.us3.log.level</name>	1
<name>fs.us3.metadata.host</name>	1
<name>fs.us3.metadata.use</name>	1
<name>fs.us3.metadata.zookeeper.addrs</name>	1
<name>fs.us3.retryTimes</name>	1
<name>fs.us3.secret.key</name>	1
<name>fs.us3.socket.recv.buffer</name>	1
<name>fs.us3.thread.pool.size</name>	1
<name>fs.us3.timeout</name>	1
<name>ha.zookeeper.quorum</name>	1
<name>hadoop.http.staticuser.user</name>	1
<name>hadoop.kms.acl.CREATE</name>	1
<name>hadoop.kms.acl.DECRYPT_EEK</name>	1
<name>hadoop.kms.acl.DELETE</name>	1
<name>hadoop.kms.acl.GENERATE_EEK</name>	1
<name>hadoop.kms.acl.GET</name>	1
<name>hadoop.kms.acl.GET_KEYS</name>	1
<name>hadoop.kms.acl.GET_METADATA</name>	1
<name>hadoop.kms.acl.ROLLOVER</name>	1
<name>hadoop.kms.acl.SET_KEY_MATERIAL</name>	1
<name>hadoop.kms.audit.aggregation.window.ms</name>	1
<name>hadoop.kms.authentication.kerberos.keytab</name>	1
<name>hadoop.kms.authentication.kerberos.name.rules</name>	1
<name>hadoop.kms.authentication.kerberos.principal</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.auth.type</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.connection.string</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.keytab</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.principal</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.path</name>	1
<name>hadoop.kms.authentication.signer.secret.provider</name>	1
<name>hadoop.kms.authentication.type</name>	1
<name>hadoop.kms.cache.enable</name>	1
<name>hadoop.kms.cache.timeout.ms</name>	1
<name>hadoop.kms.current.key.cache.timeout.ms</name>	1
<name>hadoop.kms.key.provider.uri</name>	1
<name>hadoop.proxyuser.hadoop.groups</name>	2
<name>hadoop.proxyuser.hadoop.hosts</name>	2
<name>hadoop.proxyuser.hive.groups</name>	1
<name>hadoop.proxyuser.hive.hosts</name>	1
<name>hadoop.proxyuser.hue.groups</name>	1
<name>hadoop.proxyuser.hue.hosts</name>	1
<name>hadoop.proxyuser.mapred.groups</name>	1
<name>hadoop.proxyuser.mapred.hosts</name>	1
<name>hadoop.proxyuser.oozie.groups</name>	1
<name>hadoop.proxyuser.oozie.hosts</name>	1
<name>hadoop.security.keystore.java-keystore-provider.password-file</name>	1
<name>httpfs.proxyuser.hadoop.groups</name>	1
<name>httpfs.proxyuser.hadoop.hosts</name>	1
<name>httpfs.proxyuser.hue.groups</name>	1
<name>httpfs.proxyuser.hue.hosts</name>	1
<name>io.compression.codecs</name>	1
<name>ipc.maximum.data.length</name>	1
<name>mapreduce.framework.name</name>	1
<name>mapreduce.job.reduce.slowstart.completedmaps</name>	1
<name>mapreduce.jobhistory.address</name>	1
<name>mapreduce.jobhistory.webapp.address</name>	1
<name>mapreduce.map.java.opts</name>	1
<name>mapreduce.map.memory.mb</name>	1
<name>mapreduce.reduce.java.opts</name>	1
<name>mapreduce.reduce.memory.mb</name>	1
<name>mapreduce.reduce.shuffle.parallelcopies</name>	1
<name>mapreduce.shuffle.port</name>	1
<name>mapreduce.task.io.sort.factor</name>	1
<name>mapreduce.task.io.sort.mb</name>	1
<name>q1</name>	1
<name>q2</name>	1
<name>security.admin.operations.protocol.acl</name>	1
<name>security.applicationclient.protocol.acl</name>	1
<name>security.applicationhistory.protocol.acl</name>	1
<name>security.applicationmaster.protocol.acl</name>	1
<name>security.client.datanode.protocol.acl</name>	1
<name>security.client.protocol.acl</name>	1
<name>security.containermanagement.protocol.acl</name>	1
<name>security.datanode.protocol.acl</name>	1
<name>security.ha.service.protocol.acl</name>	1
<name>security.inter.datanode.protocol.acl</name>	1
<name>security.job.client.protocol.acl</name>	1
<name>security.job.task.protocol.acl</name>	1
<name>security.mrhs.client.protocol.acl</name>	1
<name>security.namenode.protocol.acl</name>	1
<name>security.qjournal.service.protocol.acl</name>	1
<name>security.refresh.policy.protocol.acl</name>	1
<name>security.refresh.user.mappings.protocol.acl</name>	1
<name>security.resourcelocalizer.protocol.acl</name>	1
<name>security.resourcemanager-administration.protocol.acl</name>	1
<name>security.resourcetracker.protocol.acl</name>	1
<name>security.zkfc.protocol.acl</name>	1
<name>ssl.client.keystore.keypassword</name>	1
<name>ssl.client.keystore.location</name>	1
<name>ssl.client.keystore.password</name>	1
<name>ssl.client.keystore.type</name>	1
<name>ssl.client.truststore.location</name>	1
<name>ssl.client.truststore.password</name>	1
<name>ssl.client.truststore.reload.interval</name>	1
<name>ssl.client.truststore.type</name>	1
<name>ssl.server.exclude.cipher.list</name>	1
<name>ssl.server.keystore.keypassword</name>	1
<name>ssl.server.keystore.location</name>	1
<name>ssl.server.keystore.password</name>	1
<name>ssl.server.keystore.type</name>	1
<name>ssl.server.truststore.location</name>	1
<name>ssl.server.truststore.password</name>	1
<name>ssl.server.truststore.reload.interval</name>	1
<name>ssl.server.truststore.type</name>	1
<name>yarn.acl.enable</name>	1
<name>yarn.admin.acl</name>	1
<name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>	1
<name>yarn.app.mapreduce.am.staging-dir</name>	1
<name>yarn.application.classpath</name>	1
<name>yarn.log-aggregation-enable</name>	1
<name>yarn.log-aggregation.retain-check-interval-seconds</name>	1
<name>yarn.log-aggregation.retain-seconds</name>	1
<name>yarn.log.server.url</name>	1
<name>yarn.nodemanager.address</name>	1
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>	1
<name>yarn.nodemanager.aux-services</name>	1
<name>yarn.nodemanager.container-executor.class</name>	1
<name>yarn.nodemanager.linux-container-executor.group</name>	1
<name>yarn.nodemanager.local-dirs</name>	1
<name>yarn.nodemanager.localizer.address</name>	1
<name>yarn.nodemanager.log-dirs</name>	1
<name>yarn.nodemanager.remote-app-log-dir</name>	1
<name>yarn.nodemanager.resource.cpu-vcores</name>	1
<name>yarn.nodemanager.resource.memory-mb</name>	1
<name>yarn.nodemanager.vmem-check-enabled</name>	1
<name>yarn.nodemanager.webapp.address</name>	1
<name>yarn.resourcemanager.address.rm1</name>	1
<name>yarn.resourcemanager.address.rm2</name>	1
<name>yarn.resourcemanager.admin.address.rm1</name>	1
<name>yarn.resourcemanager.admin.address.rm2</name>	1
<name>yarn.resourcemanager.cluster-id</name>	1
<name>yarn.resourcemanager.connect.retry-interval.ms</name>	1
<name>yarn.resourcemanager.ha.automatic-failover.embedded</name>	1
<name>yarn.resourcemanager.ha.automatic-failover.enabled</name>	1
<name>yarn.resourcemanager.ha.enabled</name>	1
<name>yarn.resourcemanager.ha.id</name>	1
<name>yarn.resourcemanager.ha.rm-ids</name>	1
<name>yarn.resourcemanager.nodes.exclude-path</name>	1
<name>yarn.resourcemanager.recovery.enabled</name>	1
<name>yarn.resourcemanager.resource-tracker.address.rm1</name>	1
<name>yarn.resourcemanager.resource-tracker.address.rm2</name>	1
<name>yarn.resourcemanager.scheduler.address.rm1</name>	1
<name>yarn.resourcemanager.scheduler.address.rm2</name>	1
<name>yarn.resourcemanager.scheduler.class</name>	1
<name>yarn.resourcemanager.store.class</name>	1
<name>yarn.resourcemanager.system-metrics-publisher.enabled</name>	1
<name>yarn.resourcemanager.webapp.address.rm1</name>	1
<name>yarn.resourcemanager.webapp.address.rm2</name>	1
<name>yarn.resourcemanager.webapp.https.address.rm1</name>	1
<name>yarn.resourcemanager.webapp.https.address.rm2</name>	1
<name>yarn.resourcemanager.zk-address</name>	1
<name>yarn.scheduler.capacity.maximum-am-resource-percent</name>	1
<name>yarn.scheduler.capacity.maximum-applications</name>	1
<name>yarn.scheduler.capacity.node-locality-delay</name>	1
<name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>	1
<name>yarn.scheduler.capacity.queue-mappings-override.enable</name>	1
<name>yarn.scheduler.capacity.queue-mappings</name>	1
<name>yarn.scheduler.capacity.resource-calculator</name>	1
<name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>	1
<name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>	1
<name>yarn.scheduler.capacity.root.default.capacity</name>	1
<name>yarn.scheduler.capacity.root.default.maximum-capacity</name>	1
<name>yarn.scheduler.capacity.root.default.state</name>	1
<name>yarn.scheduler.capacity.root.default.user-limit-factor</name>	1
<name>yarn.scheduler.capacity.root.queues</name>	1
<name>yarn.scheduler.fair.allow-undeclared-pools</name>	1
<name>yarn.scheduler.fair.user-as-default-queue</name>	1
<name>yarn.scheduler.maximum-allocation-mb</name>	2
<name>yarn.scheduler.maximum-allocation-vcores</name>	1
<name>yarn.timeline-service.address</name>	1
<name>yarn.timeline-service.enabled</name>	1
<name>yarn.timeline-service.generic-application-history.enabled</name>	1
<name>yarn.timeline-service.handler-thread-count</name>	1
<name>yarn.timeline-service.hostname</name>	1
<name>yarn.timeline-service.http-cross-origin.enabled</name>	1
<name>yarn.timeline-service.leveldb-state-store.path</name>	1
<name>yarn.timeline-service.leveldb-timeline-store.path</name>	1
<name>yarn.timeline-service.webapp.address</name>	1
<name>yarn.timeline-service.webapp.https.address</name>	1
<properties>	2
<property	2
<property>	220
<queue>	3
<queues>	1
<state>running</state>	1
<table	1
<td><a	1
<td><xsl:value-of	2
<td>description</td>	1
<td>name</td>	1
<td>value</td>	1
<tr>	2
<value>	1
<value>#HOSTNAME#:#PORT#,...</value>	1
<value>${user.home}/kms.keytab</value>	1
<value>${yarn.timeline-service.hostname}:10200</value>	1
<value>${yarn.timeline-service.hostname}:8188</value>	1
<value>${yarn.timeline-service.hostname}:8190</value>	1
<value>*</value>	52
<value>-Xmx1843M</value>	2
<value>/data/dfs/dn</value>	1
<value>/data/dfs/jn</value>	1
<value>/data/dfs/nn</value>	1
<value>/data/yarn/local</value>	1
<value>/data/yarn/logs</value>	1
<value>/data/yarn/timeline</value>	2
<value>/etc/hadoop/conf/kms.keytab</value>	1
<value>/hadoop-kms/hadoop-auth-signature-secret</value>	1
<value>/home/hadoop/conf/excludes</value>	1
<value>/home/hadoop/conf/id_rsa</value>	1
<value>/home/hadoop/conf/yarn-excludes</value>	1
<value>/user</value>	1
<value>/var/lib/hadoop-hdfs/dn_socket</value>	1
<value>0.0.0.0:23333</value>	1
<value>0.0.0.0:23344</value>	1
<value>0.0.0.0:23999</value>	1
<value>0.1</value>	1
<value>0.95</value>	1
<value>10000</value>	4
<value>100</value>	3
<value>10922</value>	1
<value>10</value>	1
<value>12</value>	1
<value>16348</value>	1
<value>1</value>	2
<value>2000</value>	1
<value>2048</value>	2
<value>20</value>	2
<value>23080</value>	1
<value>2</value>	1
<value>30000</value>	2
<value>32</value>	1
<value>3</value>	1
<value>4096</value>	1
<value>40</value>	1
<value>45000</value>	1
<value>4</value>	1
<value>5000</value>	1
<value>50</value>	1
<value>512</value>	1
<value>5184000</value>	1
<value>5</value>	1
<value>600000</value>	1
<value>60</value>	1
<value>65535</value>	2
<value>69415731</value>	1
<value>7320</value>	1
<value>8192</value>	1
<value>86400</value>	1
<value>8</value>	1
<value>900000</value>	1
<value></value>	14
<value>DEFAULT</value>	1
<value>HTTP/localhost</value>	1
<value>RUNNING</value>	1
<value>TLS_ECDHE_RSA_WITH_RC4_128_SHA,SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,	1
<value>Ucluster</value>	1
<value>cn.ucloud.us3.fs.US3FileSystem</value>	1
<value>cn.ucloud.us3.fs.US3Fs</value>	1
<value>default</value>	1
<value>false</value>	4
<value>hadoop</value>	2
<value>hdfs://Ucluster/var/log/hadoop-yarn/apps</value>	1
<value>hdfs://Ucluster</value>	1
<value>http://uhadoop-otfiw4quo1z-master2:19888/jobhistory/logs</value>	1
<value>info</value>	1
<value>internal-cn-sh2-01.ufileos.com</value>	1
<value>jceks://file@/${user.home}/kms.keystore</value>	1
<value>jks</value>	4
<value>kms.keystore.password</value>	1
<value>kms/#HOSTNAME#</value>	1
<value>mapreduce_shuffle</value>	1
<value>nn1,nn2</value>	1
<value>none</value>	1
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>	1
<value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec</value>	1
<value>org.apache.hadoop.mapred.ShuffleHandler</value>	1
<value>org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor</value>	1
<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>	1
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>	1
<value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>	1
<value>qjournal://uhadoop-otfiw4quo1z-master1:8485;uhadoop-otfiw4quo1z-master2:8485;uhadoop-otfiw4quo1z-core1:8485/Ucluster</value>	1
<value>random</value>	1
<value>rm1,rm2</value>	1
<value>rm1</value>	1
<value>simple</value>	1
<value>sshfence(hadoop:22)</value>	1
<value>true</value>	17
<value>ucloud-yarn-rm-cluster</value>	1
<value>uhadoop-otfiw4quo1z-master1:2181,uhadoop-otfiw4quo1z-master2:2181,uhadoop-otfiw4quo1z-core1:2181</value>	3
<value>uhadoop-otfiw4quo1z-master1:23125</value>	1
<value>uhadoop-otfiw4quo1z-master1:23130</value>	1
<value>uhadoop-otfiw4quo1z-master1:23140</value>	1
<value>uhadoop-otfiw4quo1z-master1:23141</value>	1
<value>uhadoop-otfiw4quo1z-master1:23188</value>	1
<value>uhadoop-otfiw4quo1z-master1:23189</value>	1
<value>uhadoop-otfiw4quo1z-master1:50070</value>	1
<value>uhadoop-otfiw4quo1z-master1:8020</value>	1
<value>uhadoop-otfiw4quo1z-master2:10020</value>	1
<value>uhadoop-otfiw4quo1z-master2:19888</value>	1
<value>uhadoop-otfiw4quo1z-master2:23125</value>	1
<value>uhadoop-otfiw4quo1z-master2:23130</value>	1
<value>uhadoop-otfiw4quo1z-master2:23140</value>	1
<value>uhadoop-otfiw4quo1z-master2:23141</value>	1
<value>uhadoop-otfiw4quo1z-master2:23188</value>	1
<value>uhadoop-otfiw4quo1z-master2:23189</value>	1
<value>uhadoop-otfiw4quo1z-master2:50070</value>	1
<value>uhadoop-otfiw4quo1z-master2:8020</value>	1
<value>uhadoop-otfiw4quo1z-master2</value>	1
<value>yarn,mapred,hdfs,hadoop</value>	1
<value>yarn</value>	1
<xsl:for-each	1
<xsl:output	1
<xsl:stylesheet	1
<xsl:template	1
=	5
@echo	3
@rem	73
A	22
ACL	36
ACL,	2
ACLs	4
ANY	21
ANud0tsNM7h7kDRMNBdDhzvOfytHKYvDavYnqZhdpExzYO9JdWnRthBzrn4lsvwJ	1
ASF	11
AWS	1
Add	1
Admin	2
AdminOperationsProtocol.	1
Advanced	1
All	3
Apache	33
AppSummaryLogging	1
Appender	8
Application	2
ApplicationClientProtocol,	1
ApplicationHistoryProtocol,	1
ApplicationMaster	1
ApplicationMasterProtocol,	1
ApplicationMasters	2
Audit	1
Authentication	2
Automatically	2
BASIS,	21
Backend	1
Bbl7i/f53Q0LpPMai/zlLOyFTcczfQtt9SFxFjQWh6qskSfcfO0ais339mPJW68F	1
Below	1
BlockManager	1
By	1
CAN	1
CATALINA_OPTS=	2
CLASSPATH	2
CONDITIONS	21
CPU	2
CREATE	1
Cache	1
Cached	1
Can	1
CapacityScheduler	1
ClientDatanodeProtocol,	1
ClientProtocol,	1
Command	1
Complementary	1
Configuration	17
ContainerManagementProtocol	1
Controller	1
Controls	1
Counter	1
CryptoExtension	2
Currently,	1
Custom	1
DECRYPT_EEK	1
DN.	2
DOUWA9GUvl4zu0+m9jVJQUnXPz3m2GXme4QN0grU/PFD/3TTpsVVOoana6uefetf	1
Daily	2
DataNode	1
DatanodeProtocol,	1
Date	2
Debugging	2
Default	9
DefaultResourceCalculator	1
Defaults	1
Define	2
DistributedFileSystem.	1
DominantResourceCalculator	1
Duplicate	1
Embedded	2
Empty	2
Enable	1
Event	1
EventCounter	1
Expiry	2
Extra	4
Failover	1
File	2
FileSystem	1
For	25
Foundation	11
GENERATE_EEK	1
GET	2
Ganglia	7
HAAdmin	1
HADOOP_CLASSPATH	1
HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f	1
HADOOP_CLASSPATH=$f	1
HADOOP_CLASSPATH=%HADOOP_CLASSPATH%;%HADOOP_HOME%\contrib\capacity-scheduler\*.jar	1
HADOOP_CLASSPATH=%HADOOP_HOME%\contrib\capacity-scheduler\*.jar	1
HADOOP_CLIENT_OPTS="$HADOOP_CLIENT_OPTS"	1
HADOOP_CLIENT_OPTS="-Xmx512m	1
HADOOP_CLIENT_OPTS=%HADOOP_CLIENT_OPTS%	1
HADOOP_CLIENT_OPTS=-Xmx512m	1
HADOOP_CONF_DIR=	1
HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/home/hadoop/conf"}	1
HADOOP_DATANODE_HEAPSIZE=4778	1
HADOOP_DATANODE_OPTS="-server	1
HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS	1
HADOOP_HEAPSIZE	3
HADOOP_HEAPSIZE=	2
HADOOP_HEAPSIZE=1024	1
HADOOP_IDENT_STRING=$USER	1
HADOOP_IDENT_STRING=%USERNAME%	1
HADOOP_JAAS_DEBUG=true	1
HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData	1
HADOOP_JHS_LOGGER=INFO,RFA	1
HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000	2
HADOOP_JOB_HISTORYSERVER_OPTS=	1
HADOOP_LIBEXEC_DIR=/home/hadoop/libexec	1
HADOOP_LOG_DIR=$hdfs_log_dir_prefix	1
HADOOP_LOG_DIR=%HADOOP_LOG_DIR%\%USERNAME%	1
HADOOP_MAPRED_IDENT_STRING=	1
HADOOP_MAPRED_LOG_DIR=""	1
HADOOP_MAPRED_LOG_DIR=/var/log/hadoop-mapreduce	1
HADOOP_MAPRED_NICENESS=	1
HADOOP_MAPRED_PID_DIR=	1
HADOOP_MAPRED_PID_DIR=/var/run/hadoop-mapreduce	1
HADOOP_MAPRED_ROOT_LOGGER=%HADOOP_LOGLEVEL%,RFA	1
HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA	1
HADOOP_MOVER_OPTS=""	1
HADOOP_NAMENODE_HEAPSIZE=5120	1
HADOOP_NAMENODE_INIT_HEAPSIZE=""	2
HADOOP_NAMENODE_OPTS="${SHARED_HADOOP_NAMENODE_OPTS}	1
HADOOP_NAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER%	1
HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"	1
HADOOP_OPTS	3
HADOOP_OPTS="$HADOOP_OPTS	1
HADOOP_OPTS="-Djava.net.preferIPv4Stack=true	1
HADOOP_OPTS=%HADOOP_OPTS%	1
HADOOP_PID_DIR=$hadoop_pid_dir_prefix	1
HADOOP_PID_DIR=%HADOOP_PID_DIR%	1
HADOOP_PORTMAP_OPTS="-Xmx512m	1
HADOOP_SECONDARYNAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER%	1
HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}	1
HADOOP_SECURE_DN_LOG_DIR=%HADOOP_LOG_DIR%\%HADOOP_HDFS_USER%	1
HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}	1
HADOOP_SECURE_DN_PID_DIR=%HADOOP_PID_DIR%	1
HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}	1
HADOOP_SECURE_DN_USER=%HADOOP_SECURE_DN_USER%	1
HADOOP_SECURITY_LOGGER	1
HADOOP_SECURITY_LOGGER=INFO,RFAS	1
HADOOP_USER_CLASSPATH_FIRST=true	1
HADOOP_YARN_HOME=/home/hadoop	1
HADOOP_YARN_USER	1
HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}	1
HADOOP_YARN_USER=%yarn%	1
HAService	1
HDFS	4
HDFS_AUDIT_LOGGER	1
HDFS_AUDIT_LOGGER=INFO,NullAppender	1
HEAP	1
HH:mm:ss}	2
HS	1
HSClientProtocol,	1
HTTP	5
HTTPFS_ADMIN_PORT=`expr	1
HTTPFS_HTTP_HOSTNAME=`hostname	1
HTTPFS_HTTP_PORT=14000	2
HTTPFS_LOG=${HTTPFS_HOME}/logs	1
HTTPFS_LOG=/var/log/hadoop-hdfs	1
HTTPFS_SSL_ENABLED=false	1
HTTPFS_SSL_KEYSTORE_FILE=${HOME}/.keystore	1
HTTPFS_SSL_KEYSTORE_PASS=password	1
HTTPFS_TEMP=${HTTPFS_HOME}/temp	1
Hadoop	2
Hadoop's	1
Hadoop-specific	2
Heapsize	3
Here	1
History	1
Http	1
HttpFS	8
HttpFSServer	1
IFS	1
IFS=	1
IS"	21
If	19
In	1
Increasing	1
Indicates	2
InterDatanodeProtocol,	1
Irrespective	2
It	2
JAAS	1
JAVA=$JAVA_HOME/bin/java	1
JAVA_HEAP_MAX	3
JAVA_HEAP_MAX="-Xmx""$YARN_HEAPSIZE""m"	1
JAVA_HEAP_MAX=-Xmx%YARN_HEAPSIZE%m	1
JAVA_HEAP_MAX=-Xmx1000m	1
JAVA_HOME	3
JAVA_HOME.	2
JAVA_HOME=$JAVA_HOME	1
JAVA_HOME=${JAVA_HOME}	1
JAVA_HOME=%JAVA_HOME%	1
JAVA_HOME=/home/y/libexec/jdk1.6.0/	1
JAVA_HOME=/usr/java/latest	2
JAVA_LIBRARY_PATH	1
JAVA_LIBRARY_PATH=${HOME}/lib/native	1
JNs	1
JSVC_HOME=${JSVC_HOME}	1
JSVC_HOME=%JSVC_HOME%	1
JVM	3
Java	10
JavaKeyStoreProvider,	1
Jets3t	1
Job	1
JobSummary	1
Jsvc	3
KEY-----	2
KIND,	21
KMS	17
KMS.	2
KMS_ADMIN_PORT=`expr	1
KMS_HTTP_PORT=16000	1
KMS_LOG=${KMS_HOME}/logs	1
KMS_MAX_HTTP_HEADER_SIZE=65536	1
KMS_MAX_THREADS=1000	1
KMS_SSL_KEYSTORE_FILE=${HOME}/.keystore	1
KMS_SSL_KEYSTORE_PASS=password	1
KMS_TEMP=${KMS_HOME}/temp	1
Kerberos	8
KeyProvider	2
KeyProvider.	2
LHZYaX8BBZriM9bRVPhrGuM7qrs5kDai4wMG0Sz3IjjYTlqwBtFnfnnA3ncnOTtW	1
LICENSE	7
Legal	1
License	63
License,	21
License.	42
Licensed	21
Log	3
LogLevel	2
LogMessage	2
LoggerName	2
Logging	2
Logs	1
Lower	1
MANAGEMENT	1
MB,	1
MB.	5
MIIEogIBAAKCAQEAu/LqVltJLURGoCkN6P7+oIeRUEmud8XQtT+G62SrS80lWOfe	1
MR	2
MRClientProtocol,	1
Manager	2
Map/Reduce	2
Maximum	2
Memory	1
Memory,	1
Metrics.	1
Modifications	1
Mover	1
Mover.	1
Must	4
NN	3
NOT**	1
NOTE:	2
NOTICE	11
NU8CvTmQ6S9lr1HoG2cyDvy19FUE3ecFyPihayaaEmEaC24sq1SkkQ5ntjLXcLjB	1
Name	1
NameNode	1
NameNode.	1
NamenodeProtocol,	1
Node	1
NodeManager	4
NodeManager.	1
Note	2
NtomQA1kZviITZsoYLk2vhnSXBzTX1C1A6BD8Yen+UmcYW4m3kH7E+cCgYEA0Pzm	1
Null	1
Number	1
OF	21
OFF_SWITCH	2
OR	21
On	2
Only!	1
Options	1
Otherwise	2
PRIVATE	2
Parameter	1
Path	1
Pattern	4
Physical	1
Pick	2
Protocols	1
Put	3
QJournalProtocol,	1
Queue	1
QuorumJournalManager	1
READ	1
RM,	1
ROLLOVER	1
RSA	2
RUNNING	1
RefreshAuthorizationPolicyProtocol,	1
RefreshUserMappingsProtocol.	1
Request	1
Required.	1
Requires	1
Resource	1
ResourceCalculator	1
ResourceLocalizer	2
ResourceManager	8
ResourceManager.	1
ResourceManagerAdministrationProtocol,	1
ResourceTrackerProtocol,	1
Resources	1
Ro3LtVTYx5Sm/85mfbtow7g0IkVs+cZyhvWz8xPnp+PzcNSmyRU2BFndBy004HZf	1
Rolling	3
Rollover	1
Rules	1
S3A	1
SASL	2
SDK	1
SHARED_HADOOP_NAMENODE_OPTS="-server	1
SPNEGO	1
SSL	10
SSL_RSA_EXPORT_WITH_RC4_40_MD5,SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,	1
SSL_RSA_WITH_DES_CBC_SHA,SSL_DHE_RSA_WITH_DES_CBC_SHA,	1
SSL_RSA_WITH_RC4_128_MD5</value>	1
STOPPED.	1
Security	2
See	40
Sends	1
Server	2
Set	6
Settings	2
Setup	2
Software	11
Specifies	2
Specify	7
Specifying	1
State	2
Summary	3
System	4
TC0AUnj4zf2MKZzRuaDeqCuRfLqB8HjeA7Z2r++TS9jA5S4spWwwg5LafRRwneql	1
Tag	1
TaskLog	1
TaskUmbilicalProtocol,	1
The	101
These	3
This	11
Threshold	1
To	1
Tomcat	4
Typically	2
UMGUSHy9S4W6o9clMlqGZbVJAcRYSeXzLztoq6+MSNn8ZtUcpvhwtHiUougszKyy	1
URI	1
Uncomment	3
Unless	21
Use	1
Used	2
User	2
Users	1
V9AoSRpVMXQrWoexkCBbgpGYX1yfibSTGdvEQsqQrqMEX8zA/YbLwTrRVhU+YQeu	1
Version	21
ViwnAoGAIlPd4QFJ5Y1M0tQ3i5/IQZvgg7l1KTcfi9LEsXJ62aX+7rfwyl1uMifj	1
WARRANTIES	21
WITHOUT	21
WebHdfs	1
When	3
Where	5
Whether	1
XML	1
Xmx	3
YARN	4
YARN_CONF_DIR	1
YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}"	1
YARN_CONF_DIR=%HADOOP_YARN_HOME%\conf	1
YARN_HEAPMAX	6
YARN_HEAPSIZE	1
YARN_HEAPSIZE=1000	1
YARN_LOGFILE	1
YARN_LOGFILE='yarn.log'	1
YARN_LOGFILE=yarn.log	1
YARN_LOG_DIR	1
YARN_LOG_DIR="$HADOOP_YARN_HOME/logs"	1
YARN_LOG_DIR=%HADOOP_YARN_HOME%\logs	1
YARN_LOG_DIR=/var/log/hadoop-yarn	1
YARN_NODEMANAGER_HEAPSIZE=2048	1
YARN_NODEMANAGER_OPTS.	1
YARN_NODEMANAGER_OPTS=	1
YARN_OPTS	7
YARN_OPTS="$YARN_OPTS	11
YARN_OPTS=%YARN_OPTS%	11
YARN_PID_DIR=/var/run/hadoop-yarn	1
YARN_POLICYFILE	1
YARN_POLICYFILE="hadoop-policy.xml"	1
YARN_POLICYFILE=hadoop-policy.xml	1
YARN_RESOURCEMANAGER_HEAPSIZE=2048	1
YARN_RESOURCEMANAGER_OPTS.	1
YARN_RESOURCEMANAGER_OPTS=	1
YARN_ROOT_LOGGER	1
YARN_ROOT_LOGGER=%HADOOP_LOGLEVEL%,console	1
YARN_TIMELINESERVER_HEAPSIZE=1000	1
YARN_TIMELINESERVER_OPTS.	1
Yarn	1
You	29
ZK	1
ZNode	1
Zookeeper	3
Zookeeper.	2
[	9
[%X{hostname}][%X{user}:%X{doAs}]	2
[prefix].[source|sink].[instance].[options]	1
[u|g]:[name]:[queue_name][,next	1
];	9
a	89
above	3
absolute	1
accept	2
access	1
accompanying	7
acl	1
acls	5
act	1
active	1
additional	11
admin	2
administer	1
administrators	3
affects	2
after	3
aggregated	2
aggregation	1
agreed	21
agreements.	11
all	30
allocated	1
allow	1
allowed	3
allowed.</description>	18
allowed.system.users=##comma	1
allows	2
along	1
also	1
amount	2
an	29
and	92
and/or	3
any	5
app	2
appended	4
appender	3
appender)	1
applicable	21
application	1
applications	5
applications.	1
applications.</description>	1
applies	2
appropriately	1
approximately	1
are	45
args	3
as	15
assign	1
assignments	1
at	28
attack.	2
attempts	1
audit	4
authentication	7
authorization	5
available	1
backing	3
banned.users=#comma	1
be	50
behaviour	1
below	1
best	2
bind	1
binding,	1
blank.	18
block	4
border="1">	1
builtin	1
by	87
cache	2
cache,	2
cached	1
called	1
can	16
cannot	1
cap	1
capacity	1
capacity-scheduler.	2
capacity.</description>	1
case	1
change	2
changes	1
changing	2
check	5
checked	1
child	1
cipher	1
client	2
client-to-datanode	1
clients	5
cluster	5
cluster,	1
code	1
comma	1
comma-separated	18
commands	3
commands.	2
commas.	1
communciate	2
communicate	10
communication.</description>	1
compare	2
compiled	1
compliance	21
concurrent	1
config	1
config,	1
configs	1
configuration	8
configuration,	2
configuration.	2
configured	7
connect	2
connection	3
connections	1
console	1
consulting	1
contain	2
containers.	2
containers.</description>	2
context	18
contributor	11
controls	1
cookie	2
cookies	1
copy	21
copyright	11
cores	1
correctly	3
count	1
counts	1
create-key	1
creating	1
credentials	2
current	1
dUG1yVECgYEA5jpuJ59mCErITTCG+ToPwqJFEHlGOglvv/4LbkP22Mw4byLVHS8W	1
daemons	2
daemons.	3
data	7
datanode	3
datanode-metrics.log	1
datanode.metrics.logger=INFO,NullAppender	1
datanodes	3
datanodes,	2
datanodes.	1
debugging	1
debugging,	1
decryptEncryptedKey	1
default	30
default.	10
defined	21
defined.	4
defining	1
delete-key	1
deleted	1
deprecation	1
details	1
dfs,	2
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
dfs.class=org.apache.hadoop.metrics.spi.NullContext	1
dfs.period=10	1
dfs.servers=localhost:8649	1
dfsadmin	1
different	5
direct	1
directory	9
directory)	1
distcp	2
distcp.	2
distributed	55
do	6
document.	1
dominant-resource	1
done	1
dropping	2
dtnode_heapsize=${HADOOP_DATANODE_HEAPSIZE}m	1
during	1
e.g.	18
each	6
each.	2
echo	2
edit	1
eg.	1
either	28
element	1
element.	1
elements.	2
else	2
empty	2
enable	3
enabled	2
enabled,	1
encoding="UTF-8"	3
encoding="UTF-8"?>	4
end	1
endpoint.	1
environment	6
environment.	2
envvars	1
errors	1
etc)	2
etc.	3
events	2
events.	1
example	1
example,	5
except	21
excluded	1
exist	1
exit	1
explicit	1
explicitly	4
export	66
express	21
extended	1
extra	1
f	1
false	1
false.	1
feature	1
fencer.	1
fi	9
file	58
file,	2
file.	11
filename	1
filename)	1
filenames	1
files	10
files)	1
flags	3
flushed	1
following	8
for	146
format	5
format,	4
format:	2
from	9
from.	1
fsck,	2
full	1
ganglia	6
generateEncryptedKey	1
generation	1
generic	1
get-current-key	1
get-key-metadata	1
get-key-version	1
get-keys	1
get-keys-metadata	1
getCurrentKey	2
getKeyVersion	1
getKeyVersion,	1
getMetadata,	1
getMetadata.	1
governing	21
group	36
group1,group2	2
groups	2
hadoop	4
hadoop-env.sh	1
hadoop.	3
hadoop.log.dir	1
hadoop.log.dir=.	1
hadoop.log.file=hadoop.log	1
hadoop.log.maxbackupindex=20	1
hadoop.log.maxfilesize=256MB	1
hadoop.mapreduce.jobsummary.log.file	1
hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log	1
hadoop.mapreduce.jobsummary.log.maxbackupindex=20	1
hadoop.mapreduce.jobsummary.log.maxfilesize=256MB	1
hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}	1
hadoop.mapreduce.jobsummary.logger=INFO,JSA	1
hadoop.root.logger=INFO,console	1
hadoop.security.log.file=SecurityAuth-${user.name}.audit	1
hadoop.security.log.maxbackupindex=20	1
hadoop.security.log.maxfilesize=256MB	1
hadoop.security.logger=INFO,NullAppender	1
hadoop.tasklog.iscleanup=false	1
hadoop.tasklog.logsRetainHours=12	1
hadoop.tasklog.noKeepSplits=4	1
hadoop.tasklog.purgeLogSplits=true	1
hadoop.tasklog.taskid=null	1
hadoop.tasklog.totalLogFileSize=100	1
hadoop_pid_dir_prefix=/var/run/hadoop-hdfs	1
handled	1
handler	1
has	1
hdfs	1
hdfs.audit.log.maxbackupindex=20	1
hdfs.audit.log.maxfilesize=256MB	1
hdfs.audit.logger=INFO,NullAppender	1
hdfs_log_dir_prefix=/var/log/hadoop-hdfs	1
header	1
heap	4
heapsize	1
heartbeat.	1
here	2
here.	4
hierarchical	2
his/her	1
history	1
hostname	1
hostnames	1
hot-reloaded	1
how	1
href="configuration.xsl"?>	5
hs3z3ui3HKnhU6lD6Xm+GURMpT3BeHaiHBDHlMgsR6UwGaUYyzvNUrqKoIAn3GTx	1
http://www.apache.org/licenses/LICENSE-2.0	21
httpfs	4
httpfsaudit	1
i.e.	2
if	34
implementation	5
implied.	21
improve	1
in	86
in-effect.	1
include	1
information	12
insert	2
instance	3
instances	1
instances,	1
inter-datanode	1
interval,	2
invormation	1
irrespective	1
is	123
it	9
it.	1
its	2
java	3
javadoc	1
job	6
jobs	10
jobs,	1
jobs.	2
jsvc	2
jvm	3
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
jvm.period=10	1
jvm.servers=localhost:8649	1
jy4liKPajtYCHsA6bf+xezV5BW6i9wD22+dTY9ibviN1iHJ2/9P89Lcds3o0mjU7	1
key	10
key.	1
key="capacity"	1
key="user-limit"	1
keys	1
keystore	9
keytab	2
killing	1
kms	2
kms-audit	1
language	21
last	1
law	21
leaf	2
level	7
levels	2
libraries	2
library	1
license	11
licenses	11
like	3
limit	1
limitations	21
line	2
links	1
list	44
loaded	1
localhost	1
location	3
log	19
log4j.additivity.DataNodeMetricsLog=false	1
log4j.additivity.NameNodeMetricsLog=false	1
log4j.additivity.kms-audit=false	1
log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false	1
log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false	1
log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false	1
log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false	1
log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log	1
log4j.appender.DNMETRICSRFA.MaxBackupIndex=1	1
log4j.appender.DNMETRICSRFA.MaxFileSize=64MB	1
log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd	1
log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}	1
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd	1
log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}	1
log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}	1
log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}	1
log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}	1
log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender	1
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter	1
log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}	1
log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}	1
log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}	1
log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd	1
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.JSA=org.apache.log4j.RollingFileAppender	1
log4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log	1
log4j.appender.MRAUDIT.MaxBackupIndex=${mapred.audit.log.maxbackupindex}	1
log4j.appender.MRAUDIT.MaxFileSize=${mapred.audit.log.maxfilesize}	1
log4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout	1
log4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender	1
log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log	1
log4j.appender.NNMETRICSRFA.MaxBackupIndex=1	1
log4j.appender.NNMETRICSRFA.MaxFileSize=64MB	1
log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender	1
log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}	1
log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}	1
log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}	1
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log	1
log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}	1
log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}	1
log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender	1
log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}	1
log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}	1
log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}	1
log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFAS=org.apache.log4j.RollingFileAppender	1
log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}	1
log4j.appender.RMSUMMARY.MaxBackupIndex=20	1
log4j.appender.RMSUMMARY.MaxFileSize=256MB	1
log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender	1
log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}	1
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.TLA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}	1
log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}	1
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender	1
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd	1
log4j.appender.console.layout=org.apache.log4j.PatternLayout	1
log4j.appender.console.target=System.err	1
log4j.appender.console=org.apache.log4j.ConsoleAppender	1
log4j.appender.httpfs.Append=true	1
log4j.appender.httpfs.DatePattern='.'yyyy-MM-dd	1
log4j.appender.httpfs.File=${httpfs.log.dir}/httpfs.log	1
log4j.appender.httpfs.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.httpfs.layout=org.apache.log4j.PatternLayout	1
log4j.appender.httpfs=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.httpfsaudit.Append=true	1
log4j.appender.httpfsaudit.DatePattern='.'yyyy-MM-dd	1
log4j.appender.httpfsaudit.File=${httpfs.log.dir}/httpfs-audit.log	1
log4j.appender.httpfsaudit.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.httpfsaudit.layout=org.apache.log4j.PatternLayout	1
log4j.appender.httpfsaudit=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.kms-audit.Append=true	1
log4j.appender.kms-audit.DatePattern='.'yyyy-MM-dd	1
log4j.appender.kms-audit.File=${kms.log.dir}/kms-audit.log	1
log4j.appender.kms-audit.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.kms-audit.layout=org.apache.log4j.PatternLayout	1
log4j.appender.kms-audit=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.kms.Append=true	1
log4j.appender.kms.DatePattern='.'yyyy-MM-dd	1
log4j.appender.kms.File=${kms.log.dir}/kms.log	1
log4j.appender.kms.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.kms.layout=org.apache.log4j.PatternLayout	1
log4j.appender.kms=org.apache.log4j.DailyRollingFileAppender	1
log4j.category.SecurityLogger=${hadoop.security.logger}	1
log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}	1
log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}	1
log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR	1
log4j.logger.com.amazonaws=ERROR	1
log4j.logger.com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator=OFF	1
log4j.logger.httpfsaudit=INFO,	1
log4j.logger.kms-audit=INFO,	1
log4j.logger.org.apache.curator=INFO	1
log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN	1
log4j.logger.org.apache.hadoop.conf=ERROR	1
log4j.logger.org.apache.hadoop.fs.http.server=INFO,	1
log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN	1
log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}	1
log4j.logger.org.apache.hadoop.lib=INFO,	1
log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}	1
log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}	1
log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG	1
log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}	1
log4j.logger.org.apache.hadoop=INFO	1
log4j.logger.org.apache.zookeeper=INFO	1
log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR	1
log4j.rootLogger=${hadoop.root.logger},	1
log4j.rootLogger=ALL,	1
log4j.threshold=ALL	1
logger	2
logger.	1
logging	5
logging.	2
logs	3
logs.</description>	1
loops	1
made	1
make	1
manage	1
manager	3
map	2
mapping	1
mapping]*	1
mappings	1
mappings.	1
mapred	1
mapred.audit.log.maxbackupindex=20	1
mapred.audit.log.maxfilesize=256MB	1
mapred.audit.logger=INFO,NullAppender	1
mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
mapred.class=org.apache.hadoop.metrics.spi.NullContext	1
mapred.period=10	1
mapred.servers=localhost:8649	1
mapreduce.cluster.acls.enabled	3
mapreduce.cluster.administrators	2
maps	1
masters	1
match="configuration">	1
material	3
max	3
maximum	5
may	46
means	18
memory,	1
message	1
messages	2
metadata	1
method="html"/>	1
metrics	5
midnight	1
might	1
milliseconds,	1
milliseconds.	4
min.user.id=1000#Prevent	1
missed	1
modified.	1
modify	1
modifying	1
more	12
mradmin	1
ms)	1
multi-dimensional	1
multiple	4
must	1
name	4
name.	1
name="{name}"><xsl:value-of	1
namenode	1
namenode-metrics.log	1
namenode-metrics.out	1
namenode.	2
namenode.</description>	1
namenode.metrics.logger=INFO,NullAppender	1
namenode:	1
namenode_heapsize=${HADOOP_NAMENODE_HEAPSIZE}m	1
namenode_opt_maxnewsize=128m	1
namenode_opt_newsize=128m	1
names.	19
native	1
natively	1
need	1
nesting	1
new	2
no	4
node's	1
nodes	2
nodes.	3
non-privileged	2
normal	1
not	50
null	5
number	7
numerical	3
obtain	21
of	142
off	4
on	30
one	14
one:	2
only	9
operation.	3
operations	8
operations.	9
opportunities	1
option	7
optional.	2
options	10
options.	2
or	64
ordinary	1
org.apache.hadoop.metrics2	1
other	1
other.	6
others	2
overridden	4
override	5
overrides	3
owner	1
ownership.	11
package-info.java	1
parameters	4
parent	1
part	2
particular	1
password	2
password.	1
path	3
pending	1
per	1
percent	1
percentage	1
period	1
period,	1
permissions	21
picked	3
pid	3
place	1
please	1
policy	3
port	5
portable	1
ports	2
ports.	2
potential	2
preferred	3
prefix.	1
present,	1
principal	4
principal.	1
printed	1
priorities.	1
priority	1
privileged	2
privileges	1
privileges.	1
properties	6
property	10
protocol	6
protocol,	2
protocol.	2
provide	3
q1.	1
q2	2
q2.	1
qmWrvFGzJbV1z2eGXh8A9VSz7Zj/mbv7/49kVvtlVx/k28O8mOU=	1
quashed	1
query	1
queue	12
queue).	1
queue,	1
queue.	10
queues	9
queues,	1
queues.	3
rAupSCz0gRv3tsuPMHHBE/Oft1qQvbDFM2VdqNv4dPJdiLfNx0lkxjBiAgZr1MFy	1
rEGFMyPpWYrG0sJxyI8xQ5zKSoNKZ9YUKYF9zL9rGM655bDm0xZwQO4X9SzP437K	1
rack	1
rack-local	1
rate	1
recovery.	1
reduce	3
refresh	2
refreshable.	1
regarding	11
reload	2
remote	2
representing	3
request	1
required	26
resolve	2
resources	2
response.	2
restore	1
retain	2
retrieve	1
return	1
returned	2
rolling	1
rollover-key	1
root	2
rootLogger.	1
rootlogger	1
rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
rpc.class=org.apache.hadoop.metrics.spi.NullContext	1
rpc.period=10	1
rpc.servers=localhost:8649	1
run	10
running	4
running,	1
running.	1
runs	3
runtime	3
same	1
sample	1
sampling	2
scale	3
schedule	1
scheduler.	1
schedulers,	1
scheduling	3
secondary	1
seconds	1
seconds).	2
secret	3
secure	7
security	2
security.	1
segment	1
select="description"/></td>	1
select="name"/></a></td>	1
select="property">	1
select="value"/></td>	1
send	1
sending	1
separate	3
separated	20
separated.	1
server	3
service	2
service-level	2
set	65
set."	1
sets	2
setting	7
setup	1
severity	1
shfence	1
should	6
shuffle	2
shuffleHandler	1
sign	1
signature	2
similar	3
single	1
sinks	1
site-specific	3
size	1
sizes	1
so	3
softlink	1
software	21
some	2
sometimes	1
source	2
space	1
space),	2
spaces	1
special	19
specific	28
specification.	1
specified	12
specified,	3
specified.	6
specifiying	1
specify	3
specifying	2
split	1
stand-by	1
standalone="yes"?>	3
start	3
started	2
starting	3
startup	1
state	4
states	1
status	2
stopped,	1
store	1
stored	2
stored.	7
string	3
string,	1
submission	1
submit	2
submitting	1
such	1
suites	1
summary	5
super-users	1
support	2
supported	1
supports	1
supportsparse	1
symlink	2
syntax	1
syntax:	1
system	3
tag	1
tags	3
target	1
tasks	2
tasktracker.	1
template	1
temporary	2
than	1
that	22
the	351
them	1
then	9
there	2
therefore	3
this	73
threads	1
time	4
timeline	2
timeout,	1
timestamp.	1
to	170
tomcat	1
top	1
traffic.	1
transfer	4
true.	3
turn	1
two	2
two.	3
type	1
type,	1
type="text/xsl"	5
typical	1
typically	1
u:%user:%user	1
ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
ugi.class=org.apache.hadoop.metrics.spi.NullContext	1
ugi.period=10	1
ugi.servers=localhost:8649	1
uncommented	1
under	74
unset	1
up	5
updating	1
usage	2
use	28
use,	2
use.	4
used	37
used.	3
user	48
user.	2
user1,user2	2
user?	1
users	27
users,wheel".	18
uses	2
using	14
vCz8BitFZD6U4UvtL82JPYl1Do/54HJLLreTkFex4xYvPRXAfCjrg224XKVV7Wjq	1
vYfZn3opgOQ1bFtuyYNq0cxbl5+Mf38cMGCZVHHQV4EspvUEHC0kOkLIxJkU4Luo	1
value	46
value="20"/>	1
value="30"/>	1
values	6
variable	4
variables	4
version	1
version="1.0"	7
version="1.0">	1
version="1.0"?>	6
via	3
view,	1
viewing	2
w/	1
want	2
warnings	1
warnings.	1
weak	1
when	10
where	4
which	7
while	1
who	6
will	23
window	1
window,	1
with	57
within	4
without	1
work	11
writing,	21
written	2
xAVO1jxW+JrReqA2ThpxfpJb+2W4Zz7qKwF/0IqbrqKLef9m0S/QtwvoVK7To6/T	1
xmlns:xsl="http://www.w3.org/1999/XSL/Transform"	1
yarn.ewma.cleanupInterval=300	1
yarn.ewma.maxUniqueMessages=250	1
yarn.ewma.messageAgeLimitSeconds=86400	1
yarn.nodemanager.linux-container-executor.group	1
yarn.nodemanager.linux-container-executor.group=#configured	1
yarn.server.resourcemanager.appsummary.log.file	1
yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log	1
yarn.server.resourcemanager.appsummary.logger	2
yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}	1
you	26
zookeeper	1
